import pandas as pd
import numpy as np
from typing import Dict
import os
from datetime import datetime, timedelta

from news_processing import SentimentAggregator


class PatchTSTDataIntegrator:
    """
    –ò–Ω—Ç–µ–≥—Ä–∞—Ç–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è PatchTST –º–æ–¥–µ–ª–∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤.
    –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –∫–æ—Ç–∏—Ä–æ–≤–∫–∏ –∞–∫—Ü–∏–π, sentiment –∏–∑ –Ω–æ–≤–æ—Å—Ç–µ–π –∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã.
    """

    def __init__(self,
                 context_length: int = 20,
                 prediction_length: int = 1,
                 patch_length: int = 5,
                 stride: int = 2):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω—Ç–µ–≥—Ä–∞—Ç–æ—Ä–∞ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ PatchTST –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã.

        :param context_length: –î–ª–∏–Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ –¥–Ω—è—Ö
        :param prediction_length: –ì–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –¥–Ω—è—Ö
        :param patch_length: –†–∞–∑–º–µ—Ä –ø–∞—Ç—á–∞ –¥–ª—è Transformer –º–æ–¥–µ–ª–∏
        :param stride: –®–∞–≥ –º–µ–∂–¥—É –ø–∞—Ç—á–∞–º–∏
        :return: None
        """
        self.context_length = context_length
        self.prediction_length = prediction_length
        self.patch_length = patch_length
        self.stride = stride

        self.sentiment_aggregator = SentimentAggregator()

        print(f"PatchTST Data Integrator –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω:")
        print(f"Context: {context_length}, Prediction: {prediction_length}")
        print(f"Patch: {patch_length}, Stride: {stride}")

    def load_candles_data(self, data_path: str = "data/raw/",
                          filename: str = "train_candles.csv") -> pd.DataFrame:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∫–æ—Ç–∏—Ä–æ–≤–æ–∫ –∞–∫—Ü–∏–π —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö.

        :param data_path: –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –¥–∞–Ω–Ω—ã–º–∏ –∫–æ—Ç–∏—Ä–æ–≤–æ–∫
        :param filename: –ò–º—è —Ñ–∞–π–ª–∞ —Å –∫–æ—Ç–∏—Ä–æ–≤–∫–∞–º–∏
        :return: DataFrame —Å —Ü–µ–Ω–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –∏ –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π
        """
        filepath = os.path.join(data_path, filename)

        if not os.path.exists(filepath):
            print(f"–§–∞–π–ª –∫–æ—Ç–∏—Ä–æ–≤–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω: {filepath}")
            return pd.DataFrame()

        try:
            candles_df = pd.read_csv(filepath)

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
            expected_cols = ['begin', 'ticker', 'open', 'high', 'low', 'close', 'volume']
            missing_cols = [col for col in expected_cols if col not in candles_df.columns]

            if missing_cols:
                print(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–æ–ª–æ–Ω–∫–∏ –≤ –∫–æ—Ç–∏—Ä–æ–≤–∫–∞—Ö: {missing_cols}")

            if 'begin' in candles_df.columns:
                candles_df['begin'] = pd.to_datetime(candles_df['begin'])

            print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(candles_df)} –∑–∞–ø–∏—Å–µ–π –∫–æ—Ç–∏—Ä–æ–≤–æ–∫")
            if 'ticker' in candles_df.columns:
                print(f"–¢–∏–∫–µ—Ä–æ–≤: {candles_df['ticker'].nunique()}")
            if 'begin' in candles_df.columns:
                print(f"–ü–µ—Ä–∏–æ–¥: {candles_df['begin'].min()} - {candles_df['begin'].max()}")

            target_cols = ['target_return_1d', 'target_direction_1d', 'target_return_20d', 'target_direction_20d']
            found_targets = [col for col in target_cols if col in candles_df.columns]
            if found_targets:
                print(f"Target –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –Ω–∞–π–¥–µ–Ω—ã: {found_targets}")

            return candles_df

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ—Ç–∏—Ä–æ–≤–æ–∫: {e}")
            return pd.DataFrame()

    def load_sentiment_features(self, data_path: str = "data/processed/",
                                filename: str = "processed_sentiment_features.csv") -> pd.DataFrame:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö sentiment –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

        :param data_path: –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        :param filename: –ò–º—è —Ñ–∞–π–ª–∞ —Å sentiment –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
        :return: DataFrame —Å sentiment –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –ø–æ –¥–Ω—è–º –∏ —Ç–∏–∫–µ—Ä–∞–º
        """
        filepath = os.path.join(data_path, filename)

        if not os.path.exists(filepath):
            print(f"–§–∞–π–ª sentiment –Ω–µ –Ω–∞–π–¥–µ–Ω: {filepath}")
            return pd.DataFrame()

        try:
            sentiment_df = pd.read_csv(filepath)
            sentiment_df['date'] = pd.to_datetime(sentiment_df['date'])

            print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(sentiment_df)} sentiment –∑–∞–ø–∏—Å–µ–π")
            print(f"–ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(sentiment_df.columns)}")
            if 'ticker' in sentiment_df.columns:
                print(f"–¢–∏–∫–µ—Ä–æ–≤: {sentiment_df['ticker'].nunique()}")

            corporate_features = [col for col in sentiment_df.columns if 'corporate' in col.lower()]
            if corporate_features:
                print(f"–ö–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {len(corporate_features)}")

            return sentiment_df

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ sentiment: {e}")
            return pd.DataFrame()

    def merge_data_for_patchtst(self,
                                candles_df: pd.DataFrame,
                                sentiment_df: pd.DataFrame) -> pd.DataFrame:
        """
        –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ü–µ–Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ sentiment –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è PatchTST —Å –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤.

        :param candles_df: DataFrame —Å –∫–æ—Ç–∏—Ä–æ–≤–∫–∞–º–∏ –∞–∫—Ü–∏–π
        :param sentiment_df: DataFrame —Å sentiment –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
        :return: –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π DataFrame —Å–æ –≤—Å–µ–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        """
        if candles_df.empty:
            print("–ü—É—Å—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ –∫–æ—Ç–∏—Ä–æ–≤–æ–∫")
            return pd.DataFrame()

        print("üîó –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è PatchTST...")

        candles_prepared = candles_df.copy()
        if 'begin' in candles_prepared.columns:
            candles_prepared['date'] = candles_prepared['begin'].dt.date
        else:
            print("–ö–æ–ª–æ–Ω–∫–∞ 'begin' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –∫–æ—Ç–∏—Ä–æ–≤–∫–∞—Ö")
            return pd.DataFrame()

        if sentiment_df.empty:
            print("–ù–µ—Ç sentiment –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –∫–æ—Ç–∏—Ä–æ–≤–∫–∏ —Å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º–∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º–∏")
            merged_data = candles_prepared
        else:
            sentiment_prepared = sentiment_df.copy()
            sentiment_prepared['date'] = sentiment_prepared['date'].dt.date

            if 'ticker' in candles_prepared.columns and 'ticker' in sentiment_prepared.columns:
                merged_data = candles_prepared.merge(
                    sentiment_prepared,
                    on=['date', 'ticker'],
                    how='left'
                )
            else:
                print("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–æ–ª–æ–Ω–∫–∞ 'ticker' –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è")
                merged_data = candles_prepared

            sentiment_columns = [col for col in merged_data.columns
                                 if any(keyword in col.lower() for keyword in
                                        ['sentiment', 'news', 'corporate', 'confidence'])]

            for col in sentiment_columns:
                if merged_data[col].dtype in ['int64', 'float64']:
                    merged_data[col] = merged_data[col].fillna(0.0)

        merged_data = self._add_technical_indicators(merged_data)

        print(f"–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {len(merged_data)} –∑–∞–ø–∏—Å–µ–π")

        numeric_features = len([c for c in merged_data.columns if merged_data[c].dtype in ['int64', 'float64']])
        if not sentiment_df.empty:
            sentiment_features = len([c for c in merged_data.columns if 'sentiment' in c.lower()])
            corporate_features = len([c for c in merged_data.columns if 'corporate' in c.lower()])
            print(f"–í—Å–µ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {numeric_features}")
            print(f"Sentiment –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {sentiment_features}")
            print(f"–ö–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {corporate_features}")
        else:
            print(f"–í—Å–µ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {numeric_features} (—Ç–æ–ª—å–∫–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ)")

        return merged_data

    def _add_technical_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–∫–µ—Ä–∞ –æ—Ç–¥–µ–ª—å–Ω–æ.

        :param df: DataFrame —Å —Ü–µ–Ω–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        :return: DataFrame —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º–∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º–∏
        """
        df_enhanced = df.copy()

        print("–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤...")

        if 'close' not in df.columns:
            print("–ö–æ–ª–æ–Ω–∫–∞ 'close' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã")
            return df_enhanced

        if 'ticker' not in df.columns:
            print("–ö–æ–ª–æ–Ω–∫–∞ 'ticker' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –ø—Ä–∏–º–µ–Ω—è–µ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –∫–æ –≤—Å–µ–º –¥–∞–Ω–Ω—ã–º")
            df_enhanced = self._calculate_technical_indicators(df_enhanced)
        else:
            # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–∫–µ—Ä–∞ –æ—Ç–¥–µ–ª—å–Ω–æ
            for ticker in df['ticker'].unique():
                mask = df_enhanced['ticker'] == ticker
                ticker_data = df_enhanced[mask].copy()

                if 'begin' in ticker_data.columns:
                    ticker_data = ticker_data.sort_values('begin')

                if len(ticker_data) == 0:
                    continue

                # –†–∞—Å—Å—á—ë—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
                ticker_data_enhanced = self._calculate_technical_indicators(ticker_data)

                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–π DataFrame
                for col in ticker_data_enhanced.columns:
                    if col not in df_enhanced.columns:
                        df_enhanced[col] = 0.0
                    df_enhanced.loc[mask, col] = ticker_data_enhanced[col].values

        numeric_cols = df_enhanced.select_dtypes(include=[np.number]).columns
        df_enhanced[numeric_cols] = df_enhanced[numeric_cols].fillna(0)

        print(f"–î–æ–±–∞–≤–ª–µ–Ω—ã —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã")

        return df_enhanced

    def _calculate_technical_indicators(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        –†–∞—Å—á–µ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ –¥–ª—è –æ–¥–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä—è–¥–∞.

        :param data: DataFrame —Å —Ü–µ–Ω–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –æ–¥–Ω–æ–≥–æ –∞–∫—Ç–∏–≤–∞
        :return: DataFrame —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º–∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º–∏
        """
        df = data.copy()

        if 'close' not in df.columns:
            return df

        try:
            # –°–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ —Ü–µ–Ω—ã
            df['sma_5'] = df['close'].rolling(5, min_periods=1).mean()
            df['sma_20'] = df['close'].rolling(20, min_periods=1).mean()

            # –î–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏
            df['return_1d'] = df['close'].pct_change()
            df['return_5d'] = df['close'].pct_change(5)

            # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
            df['volatility'] = df['return_1d'].rolling(10, min_periods=1).std()

            # RSI
            delta = df['close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=14, min_periods=1).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=14, min_periods=1).mean()
            rs = gain / (loss + 1e-8)
            df['rsi'] = 100 - (100 / (1 + rs))

            # MACD
            ema_12 = df['close'].ewm(span=12).mean()
            ema_26 = df['close'].ewm(span=26).mean()
            df['macd'] = ema_12 - ema_26
            df['macd_signal'] = df['macd'].ewm(span=9).mean()

            bb_window = 20
            df['bb_middle'] = df['close'].rolling(bb_window, min_periods=1).mean()
            bb_std = df['close'].rolling(bb_window, min_periods=1).std()
            df['bb_upper'] = df['bb_middle'] + (bb_std * 2)
            df['bb_lower'] = df['bb_middle'] - (bb_std * 2)
            df['bb_position'] = (df['close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'] + 1e-8)

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤: {e}")

        return df

    def create_patchtst_sequences(self, merged_data: pd.DataFrame,
                                  target_column: str = 'close') -> Dict:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è PatchTST –º–æ–¥–µ–ª–∏.

        :param merged_data: –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
        :param target_column: –ù–∞–∑–≤–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è
        :return: –°–ª–æ–≤–∞—Ä—å —Å –º–∞—Å—Å–∏–≤–∞–º–∏ X, y, —Ç–∏–∫–µ—Ä–∞–º–∏ –∏ –¥–∞—Ç–∞–º–∏
        """
        print(f"–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è PatchTST...")
        print(f"Target –∫–æ–ª–æ–Ω–∫–∞: {target_column}")

        if merged_data.empty:
            return {'X': [], 'y': [], 'tickers': [], 'dates': []}

        exclude_columns = [
            'date', 'begin', 'ticker', target_column,
            'target_return_1d', 'target_direction_1d',
            'target_return_20d', 'target_direction_20d'
        ]

        feature_columns = [col for col in merged_data.columns
                           if merged_data[col].dtype in ['int64', 'float64'] and
                           col not in exclude_columns]

        print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(feature_columns)}")

        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        price_features = [col for col in feature_columns if col in ['open', 'high', 'low', 'close', 'volume']]
        technical_features = [col for col in feature_columns if
                              any(tech in col for tech in ['sma', 'rsi', 'macd', 'bb', 'volatility', 'return'])]
        sentiment_features = [col for col in feature_columns if 'sentiment' in col.lower()]
        corporate_features = [col for col in feature_columns if 'corporate' in col.lower()]

        print(f"–¶–µ–Ω–æ–≤—ã–µ: {len(price_features)}, –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ: {len(technical_features)}")
        print(f"Sentiment: {len(sentiment_features)}, –ö–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–µ: {len(corporate_features)}")

        sequences = {'X': [], 'y': [], 'tickers': [], 'dates': []}

        if target_column not in merged_data.columns:
            print(f"Target –∫–æ–ª–æ–Ω–∫–∞ {target_column} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –¥–∞–Ω–Ω—ã—Ö")
            available_targets = [col for col in merged_data.columns if 'target' in col.lower() or col in ['close']]
            if available_targets:
                print(f"–î–æ—Å—Ç—É–ø–Ω—ã–µ target –∫–æ–ª–æ–Ω–∫–∏: {available_targets}")
                target_column = available_targets[0]
                print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º: {target_column}")
            else:
                return sequences

        if 'ticker' in merged_data.columns:
            tickers_to_process = merged_data['ticker'].unique()
        else:
            tickers_to_process = ['ALL_DATA']
            merged_data = merged_data.copy()
            merged_data['ticker'] = 'ALL_DATA'

        for ticker in tickers_to_process:
            ticker_data = merged_data[merged_data['ticker'] == ticker].copy()

            if 'begin' in ticker_data.columns:
                ticker_data = ticker_data.sort_values('begin').reset_index(drop=True)
            else:
                ticker_data = ticker_data.reset_index(drop=True)

            min_length = self.context_length + self.prediction_length
            if len(ticker_data) < min_length:
                print(f"{ticker}: –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö ({len(ticker_data)} < {min_length})")
                continue

            valid_sequences = 0
            for i in range(len(ticker_data) - min_length + 1):
                try:
                    X_sequence = ticker_data[feature_columns].iloc[
                        i:i + self.context_length
                    ].values

                    y_sequence = ticker_data[target_column].iloc[
                        i + self.context_length:i + self.context_length + self.prediction_length
                    ].values

                    if (not np.isnan(X_sequence).any() and not np.isinf(X_sequence).any() and
                            not np.isnan(y_sequence).any() and not np.isinf(y_sequence).any() and
                            X_sequence.shape[0] == self.context_length and
                            len(y_sequence) == self.prediction_length):

                        sequences['X'].append(X_sequence)
                        sequences['y'].append(y_sequence)
                        sequences['tickers'].append(ticker)

                        # –î–∞—Ç–∞
                        if 'begin' in ticker_data.columns:
                            date_val = ticker_data['begin'].iloc[i + self.context_length]
                        else:
                            date_val = i + self.context_length
                        sequences['dates'].append(date_val)

                        valid_sequences += 1

                except Exception as e:
                    continue

            if valid_sequences > 0:
                print(f"{ticker}: {valid_sequences} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π")

        if sequences['X']:
            try:
                sequences['X'] = np.array(sequences['X'])
                sequences['y'] = np.array(sequences['y'])

                print(f"–°–æ–∑–¥–∞–Ω–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π: {len(sequences['X'])}")
                print(f"–§–æ—Ä–º–∞ X: {sequences['X'].shape}")
                print(f"–§–æ—Ä–º–∞ y: {sequences['y'].shape}")

                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º –∏ target
                X_stats = {
                    'mean': np.mean(sequences['X']),
                    'std': np.std(sequences['X']),
                    'min': np.min(sequences['X']),
                    'max': np.max(sequences['X'])
                }

                y_stats = {
                    'mean': np.mean(sequences['y']),
                    'std': np.std(sequences['y']),
                    'min': np.min(sequences['y']),
                    'max': np.max(sequences['y'])
                }

                print(f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ X: mean={X_stats['mean']:.4f}, std={X_stats['std']:.4f}")
                print(f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ y: mean={y_stats['mean']:.4f}, std={y_stats['std']:.4f}")

            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –º–∞—Å—Å–∏–≤–æ–≤: {e}")
                return {'X': [], 'y': [], 'tickers': [], 'dates': []}
        else:
            print("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")

        return sequences

    def get_patchtst_config(self, sequences: Dict, target_column: str = 'close') -> Dict:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ PatchTST –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

        :param sequences: –°–ª–æ–≤–∞—Ä—å —Å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è–º–∏ X, y
        :param target_column: –ù–∞–∑–≤–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –∫–æ–ª–æ–Ω–∫–∏
        :return: –°–ª–æ–≤–∞—Ä—å —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π –º–æ–¥–µ–ª–∏
        """
        if not sequences or not sequences.get('X') or len(sequences['X']) == 0:
            print("–ù–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
            return {}

        X = sequences['X']
        y = sequences['y']
        n_features = X.shape[-1] if len(X.shape) > 2 else 0

        config = {
            'context_length': self.context_length,
            'prediction_length': self.prediction_length,
            'patch_length': min(self.patch_length, self.context_length),
            'stride': min(self.stride, self.patch_length),

            'num_input_channels': n_features,
            'd_model': 128,
            'num_attention_heads': 8,
            'num_hidden_layers': 3,
            'intermediate_size': 256,

            'dropout': 0.1,
            'attention_dropout': 0.1,
            'path_dropout': 0.1,

            'activation_function': 'gelu',

            'pooling_type': 'mean',
            'norm_type': 'batchnorm',
            'positional_encoding': 'sincos',

            'learning_rate': 0.001,
            'batch_size': 32,
            'num_epochs': 100,
            'early_stopping_patience': 10,

            'target_column': target_column,
            'target_statistics': {
                'mean': float(np.mean(y)) if len(y) > 0 else 0.0,
                'std': float(np.std(y)) if len(y) > 0 else 1.0,
                'min': float(np.min(y)) if len(y) > 0 else -1.0,
                'max': float(np.max(y)) if len(y) > 0 else 1.0
            }
        }

        print("–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è PatchTST:")
        key_params = ['context_length', 'prediction_length', 'patch_length', 'num_input_channels', 'd_model',
                      'target_column']
        for key in key_params:
            if key in config:
                print(f"   {key}: {config[key]}")

        print(
            f"   Target —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: mean={config['target_statistics']['mean']:.4f}, std={config['target_statistics']['std']:.4f}")

        return config

    def save_prepared_data(self, sequences: Dict, config: Dict,
                           output_path: str = "data/processed/") -> bool:
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.

        :param sequences: –°–ª–æ–≤–∞—Ä—å —Å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è–º–∏ X, y, —Ç–∏–∫–µ—Ä–∞–º–∏ –∏ –¥–∞—Ç–∞–º–∏
        :param config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
        :param output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤
        :return: True –µ—Å–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ, False –∏–Ω–∞—á–µ
        """
        try:
            os.makedirs(output_path, exist_ok=True)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            if sequences and sequences.get('X') is not None:
                sequences_file = os.path.join(output_path, 'patchtst_sequences.npz')
                np.savez(
                    sequences_file,
                    X=sequences['X'],
                    y=sequences['y'],
                    tickers=np.array(sequences['tickers']),
                    dates=np.array(sequences['dates'])
                )
                print(f"–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {sequences_file}")

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            if config:
                config_file = os.path.join(output_path, 'patchtst_config.json')
                import json
                with open(config_file, 'w') as f:
                    json.dump(config, f, indent=2, default=str)
                print(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {config_file}")

            return True

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
            return False


def prepare_data_for_patchtst(candles_path: str = "data/raw/train_candles.csv",
                              sentiment_path: str = "data/processed/processed_sentiment_features.csv",
                              output_path: str = "data/processed/",
                              target_column: str = None) -> Dict:
    """
    –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è PatchTST –º–æ–¥–µ–ª–∏.
    –ö–æ–æ—Ä–¥–∏–Ω–∏—Ä—É–µ—Ç –≤–µ—Å—å –ø—Ä–æ—Ü–µ—Å—Å –æ—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥–æ—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

    :param candles_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –∫–æ—Ç–∏—Ä–æ–≤–∫–∞–º–∏ –∞–∫—Ü–∏–π
    :param sentiment_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å sentiment –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
    :param output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    :param target_column: –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –µ—Å–ª–∏ None)
    :return: –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö
    """
    print("–ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –î–õ–Ø PATCHTST - –ß–ê–°–¢–¨ B")
    print("=" * 60)

    integrator = PatchTSTDataIntegrator(
        context_length=20,  # 20 –¥–Ω–µ–π –∏—Å—Ç–æ—Ä–∏–∏
        prediction_length=1,  # –ü—Ä–æ–≥–Ω–æ–∑ –Ω–∞ 1 –¥–µ–Ω—å
        patch_length=5,  # –ü–∞—Ç—á–∏ –ø–æ 5 –¥–Ω–µ–π
        stride=2  # –®–∞–≥ 2 –¥–Ω—è
    )

    print("\nüìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö:")
    candles_df = integrator.load_candles_data(
        data_path=os.path.dirname(candles_path) if os.path.dirname(candles_path) else "data/raw/",
        filename=os.path.basename(candles_path)
    )

    sentiment_df = integrator.load_sentiment_features(
        data_path=os.path.dirname(sentiment_path) if os.path.dirname(sentiment_path) else "data/processed/",
        filename=os.path.basename(sentiment_path)
    )

    if candles_df.empty:
        print("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ—Ç–∏—Ä–æ–≤–∫–∏")
        return {}

    if sentiment_df.empty:
        print("Sentiment –¥–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –∫–æ—Ç–∏—Ä–æ–≤–∫–∏ + —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã")
        sentiment_df = pd.DataFrame()

    if target_column is None:
        potential_targets = [col for col in candles_df.columns if 'target' in col.lower()]
        if potential_targets:
            target_column = potential_targets[0]
            print(f"–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±—Ä–∞–Ω target: {target_column}")
        else:
            target_column = 'close'  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
            print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º target –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: {target_column}")

    print("\n–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:")
    merged_data = integrator.merge_data_for_patchtst(candles_df, sentiment_df)

    if merged_data.empty:
        print("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å –¥–∞–Ω–Ω—ã–µ")
        return {}

    print("\n–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è PatchTST:")
    sequences = integrator.create_patchtst_sequences(merged_data, target_column)

    if not sequences or len(sequences.get('X', [])) == 0:
        print("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")
        return {}

    print("\n–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏:")
    config = integrator.get_patchtst_config(sequences, target_column)

    print("\n–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")
    success = integrator.save_prepared_data(sequences, config, output_path)

    if success:
        print("\n –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –ó–ê–í–ï–†–®–ï–ù–ê –£–°–ü–ï–®–ù–û!")
        print(f"–ì–æ—Ç–æ–≤–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è PatchTST –º–æ–¥–µ–ª–∏:")
        print(f"–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π: {len(sequences['X'])}")
        print(f"–ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {sequences['X'].shape[-1]}")
        print(f"–¢–∏–∫–µ—Ä–æ–≤: {len(set(sequences['tickers']))}")
        print(f"Target: {target_column}")

        print(f"\nSUPERVISED LEARNING –ì–û–¢–û–í:")
        print(f"X.shape: {sequences['X'].shape}")
        print(f"y.shape: {sequences['y'].shape}")
    else:
        print("\n–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö")

    return {
        'sequences': sequences,
        'config': config,
        'merged_data': merged_data,
        'success': success,
        'target_column': target_column
    }


if __name__ == "__main__":
    result = prepare_data_for_patchtst()
    if result.get('success', False):
        print("–î–∞–Ω–Ω—ã–µ –¥–ª—è PatchTST –≥–æ—Ç–æ–≤—ã!")
    else:
        print("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–µ —É–¥–∞–ª–∞—Å—å")